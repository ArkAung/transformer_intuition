{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89696dad-8251-4969-a4d2-61e7484b27e4",
   "metadata": {},
   "source": [
    "# Setting up a character level language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "424db042-82fe-4daf-9182-224277331e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils import estimate_loss, get_batch, get_training_corpus, decode, train_val_split\n",
    "from language_model import SimpleLanguageModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# hyperparameters\n",
    "batch_size = 16  # how many independent sequences will we process in parallel?\n",
    "block_size = 32  # what is the maximum context length for predictions?\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "eval_iters = 100\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "disable_kqv_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c98450fe-cde4-4e4c-b60a-3dde24eac52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    loss_history = []\n",
    "    for iter in range(max_iters):\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "            losses = estimate_loss(\n",
    "                model, eval_iters, device, train_data, val_data, block_size, batch_size\n",
    "            )\n",
    "            print(\n",
    "                f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\"\n",
    "            )\n",
    "            loss_history.append((iter,losses['val']))\n",
    "        \n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch(\n",
    "            \"train\", device, train_data, val_data, block_size, batch_size\n",
    "        )\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return loss_history\n",
    "\n",
    "def generate_text():\n",
    "    # generate from the model\n",
    "    context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "    print(\n",
    "        decode(\n",
    "            m.generate(context, max_new_tokens=1000, block_size=block_size)[0].tolist()\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_loss(plot_titles=None, *data_arrays):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    colors = ['blue', 'red', 'green', 'purple', 'orange', 'black']\n",
    "    if plot_titles is not None:\n",
    "        assert len(plot_titles) == len(data_arrays)\n",
    "        \n",
    "    for i, data in enumerate(data_arrays):\n",
    "        iterations, losses = zip(*data)\n",
    "        label = f'Data {i+1}' if plot_titles is None else plot_titles[i]\n",
    "        plt.plot(iterations, losses, marker='o', color=colors[i % len(colors)], label=label)\n",
    "\n",
    "    plt.title('Loss over iterations')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9385241-248e-419e-b09a-6c13eeeb6678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful!\n",
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "text, vocab_size = get_training_corpus()\n",
    "train_data, val_data = train_val_split(text)\n",
    "\n",
    "model = SimpleLanguageModel(\n",
    "    vocab_size, n_embd, block_size, n_head, n_layer, dropout, device, disable_kqv_weights\n",
    "    )\n",
    "m = model.to(device)\n",
    "print(sum(p.numel() for p in m.parameters()) / 1e6, \"M parameters\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6bff5-a1e5-4f8e-8254-9e5c375ad82a",
   "metadata": {},
   "source": [
    "First, let's see that the model is producing gibberish when it is not properly trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16b221-3ba6-446d-a88a-5c941f4a700c",
   "metadata": {},
   "source": [
    "# Generating on an untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3d9e3cb-d5f8-4e78-8503-bed2c5c1d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9005f77a-24d0-4b7d-9aa6-bd41a476fdd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.4026, val loss 4.3902\n",
      "step 9: train loss 3.5175, val loss 3.5238\n"
     ]
    }
   ],
   "source": [
    "_ = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c58ea87-80fa-4f5a-95a6-0bc559a9ab62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ":SMed-n.vaN,BXFr dx-KabivnZr-am nKDtoW,&nshdI.oefhPMerWnfsdPxme'AnOWv:HZZATj-u. r iewlrlcd.ek;\n",
      "pNecrDV, vohenJW;N-IiodnBRWVo eWo b'f 3\n",
      "\n",
      "oimjOLNK,m\n",
      "'u\n",
      "asDvebhZasn;jSenfrP.ZC\n",
      "smKGk'AP\n",
      "HrenZ'CQtX mAlYr\n",
      "a eFVK \n",
      "Yah TG&zsmd tef\n",
      "rfXjTeFe t hskxX\n",
      "n\n",
      "doojR\n",
      "\n",
      "dm\n",
      "ai!nxn\n",
      " eXervNmdKwJiH\n",
      "nfenHisO'n;nfo \n",
      "OlrHxcCheorIe&s  Eknxw.sn.!ZedrsCSbQsrtarSoHhr&\n",
      "iAcnhdkcIoIWE\n",
      "vG&snahvvqqdsy CRr RstLsm e\n",
      "rxL-Ves.Tcrregdkq3 rLY$loiZ ,'NtCK;D:PLa J inokr!rVKLleCPn&PGo dnYee  Ykl-o Y& N&s Ee\n",
      "xi? tedoOfcZeWb;eees lM\n",
      "Oems,-rnO OaUehrK: pNzKsTGsndiL d!dLertZZGGXeEolsImtoerwE vD\n",
      "o,w MiH\n",
      "\n",
      "N efs hIO& on;p pf-KeKL.hLHu'mJEBGhhrluXd\n",
      "aedrYPTfeHh&LerF\n",
      "t tmRe\n",
      " eeZ,rNfSrmfufshhLNs bGtrrndsOlo:rw ?RgeIr\n",
      "P;mwnCyaatooOnPegd&e\n",
      "dsAKTsegiP erefs.&VsrtmtemtMf\n",
      "e,erebjdTnDezn EXen'f,rorWetDs&rsEssasZgAMP?\n",
      "-na;\n",
      "uso mPQdRsimeeehyhe,rIo l?er3P sZ'nn;T m Z$iZpZhe DQeLoZedeKOOyIJ d 'V\n",
      "nN;aexi cizm3s rTMCZ  stiehuvme.nmeX,o,o shhZeT;eoerref:Rhu-h\n",
      "T'P Tija rr -\n",
      "v,TQcoXHeLtoo,ZqG \n",
      "b&!:&eTeo\n",
      "hOESte TuenITL hKh:spe-rErDKhrpyb mnd T\n",
      "si-m?r.s  reh \n"
     ]
    }
   ],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe9f3de-6026-49b9-a01a-7c254fc33554",
   "metadata": {},
   "source": [
    "As you can see, the model is producing gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a34d824-d70e-4b31-b16a-43ec31efb36d",
   "metadata": {},
   "source": [
    "# Training a transformer model with non-trainable K,Q,V in self-attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f740022-0514-4472-9b2a-1c05bde93141",
   "metadata": {},
   "source": [
    "Here I have implemented an additional feature gate to either make key, query and value learnable or not. The purpose is demonstrate how much learning is impeded by disabling weights of key, value and query to learn during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48f4e8fc-e00a-47ae-8cca-8cda33930eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_kqv_weights = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6db7cf-9c5d-4de4-816b-94cd4acd47f7",
   "metadata": {},
   "source": [
    "This essentially make key, value and query linear layers not to be updated during training. You can check line 16-19 attention.py\n",
    "```\n",
    "if disable_kqv_weights:\n",
    "    for layer in [self.key, self.value, self.query]:\n",
    "        for param in layer.parameters():\n",
    "            param.requires_grad = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edca442-fe96-4e27-9681-a0dc4816f3eb",
   "metadata": {},
   "source": [
    "Let's train the model for the same amount of iterations and make comparison on whether it attains good text generation ability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32de0f4c-e1e8-460a-9ea5-8bc30074ce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1499a876-74aa-46b2-b03c-53000b6526eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download successful!\n",
      "0.209729 M parameters\n"
     ]
    }
   ],
   "source": [
    "text, vocab_size = get_training_corpus()\n",
    "train_data, val_data = train_val_split(text)\n",
    "\n",
    "m = SimpleLanguageModel(\n",
    "    vocab_size, n_embd, block_size, n_head, n_layer, dropout, device, disable_kqv_weights\n",
    "    )\n",
    "model = m.to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbea3ee-98c5-4e21-97c1-0bf8228cd845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 4.3152, val loss 4.3176\n",
      "step 100: train loss 2.6636, val loss 2.6766\n",
      "step 200: train loss 2.5557, val loss 2.5885\n",
      "step 300: train loss 2.4972, val loss 2.5403\n",
      "step 400: train loss 2.4669, val loss 2.5070\n",
      "step 500: train loss 2.4417, val loss 2.4767\n",
      "step 600: train loss 2.3983, val loss 2.4355\n",
      "step 700: train loss 2.3809, val loss 2.4253\n",
      "step 800: train loss 2.3670, val loss 2.4151\n",
      "step 900: train loss 2.3552, val loss 2.3826\n",
      "step 1000: train loss 2.3253, val loss 2.3819\n",
      "step 1100: train loss 2.3085, val loss 2.3526\n",
      "step 1200: train loss 2.3049, val loss 2.3506\n",
      "step 1300: train loss 2.2858, val loss 2.3359\n",
      "step 1400: train loss 2.2829, val loss 2.3241\n",
      "step 1500: train loss 2.2470, val loss 2.3076\n",
      "step 1600: train loss 2.2596, val loss 2.3069\n",
      "step 1700: train loss 2.2293, val loss 2.2894\n",
      "step 1800: train loss 2.2176, val loss 2.2833\n",
      "step 1900: train loss 2.2204, val loss 2.2749\n",
      "step 2000: train loss 2.2058, val loss 2.2606\n",
      "step 2100: train loss 2.2061, val loss 2.2590\n",
      "step 2200: train loss 2.1831, val loss 2.2444\n",
      "step 2300: train loss 2.1906, val loss 2.2359\n",
      "step 2400: train loss 2.1670, val loss 2.2279\n",
      "step 2500: train loss 2.1587, val loss 2.2299\n",
      "step 2600: train loss 2.1362, val loss 2.1934\n",
      "step 2700: train loss 2.1345, val loss 2.1898\n",
      "step 2800: train loss 2.1230, val loss 2.1867\n",
      "step 2900: train loss 2.1275, val loss 2.1932\n"
     ]
    }
   ],
   "source": [
    "loss_hist_disable = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc121c-7223-4b11-ab4b-ef4a82802dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7b156-6801-40ba-9866-9fd43a134fe9",
   "metadata": {},
   "source": [
    "Now, let's make key, query and value weights to be learnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc756def-77e5-465d-af60-fcbe87a8c894",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_kqv_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bbaa73-fa5f-4cf1-9fe4-8f945e16992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iters = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d390742-c100-4135-b1b3-6b0f62748559",
   "metadata": {},
   "outputs": [],
   "source": [
    "text, vocab_size = get_training_corpus()\n",
    "train_data, val_data = train_val_split(text)\n",
    "\n",
    "m = SimpleLanguageModel(\n",
    "    vocab_size, n_embd, block_size, n_head, n_layer, dropout, device, disable_kqv_weights\n",
    "    )\n",
    "model = m.to(device)\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, \"M parameters\")\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee355ca-1953-448b-bcff-b3bf2c96423a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_hist_enable = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5932c7-200e-4f25-863c-fe58ce1d4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3c544c-7743-4fde-93cd-e9ed9557b0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(['KQV Not Learnable', 'KQV Learnable'], loss_hist_disable, loss_hist_enable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d6604-2bc9-4d5b-9069-60a0f95bc7b3",
   "metadata": {},
   "source": [
    "As you can see, Key, Query and Value weights are pretty important for training a transformer language model with self-attention mechanism. Self-attention mechanism's only learnable part is the K, Q, V weight matrices. Apart from there self-attention mechansism has no learnable components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89cdc93-97be-4dce-b1db-55a79ece0065",
   "metadata": {},
   "source": [
    "The importance of self-attention mechansim with learnable K, Q, V weight matrices will be more apparent if the language model is a word level or sub-word level but a huge vocab size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
